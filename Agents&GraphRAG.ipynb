{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MElV2aFFK5Lq"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KoltonHauck/Agents_and_GraphRAG/blob/main/Agents%26GraphRAG.ipynb)\n",
        "\n",
        "[Other Notebook](https://github.com/KoltonHauck/BMI6016_VectorDB/blob/main/BMI6016-VectorDB.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtDJGOem3nPZ"
      },
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkiFA7e53ZDw"
      },
      "outputs": [],
      "source": [
        "pip install openai numpy pandas scikit-learn sentence-transformers rank-bm25 faiss-cpu neo4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBgd7kNK--K"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "import neo4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tdaosZCLZ1B"
      },
      "source": [
        "# Embedding Methods\n",
        "\n",
        "Convert text (or anything) into a vector representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JFi6G_LQq9e"
      },
      "outputs": [],
      "source": [
        "# Sample corpus\n",
        "documents = [\n",
        "    \"The patient presents with acute chest pain and shortness of breath. An ECG was performed, showing ST elevation in leads II, III, and aVF.\",\n",
        "    \"The patient experienced minor discomfort in the chest, but ECG results were normal, ruling out myocardial infarction.\",\n",
        "    \"Aspirin and nitroglycerin were administered to manage angina symptoms before transfer to the cardiac unit.\",\n",
        "    \"The cardiologist performed an angioplasty with stent placement in the right coronary artery.\",\n",
        "    \"The patient complained of chest tightness and a persistent cough, with imaging confirming pneumonia.\",\n",
        "    \"Diabetes mellitus with uncontrolled hyperglycemia required insulin therapy adjustment.\"\n",
        "]\n",
        "\n",
        "# BM25 Embeddings\n",
        "tokenized_corpus = [doc.split(\" \") for doc in documents]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "bm25_scores = {doc: bm25.get_scores(doc.split(\" \")) for doc in documents}\n",
        "\n",
        "# TF-IDF Embeddings\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents).toarray()\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "def get_tfidf_vector(doc):\n",
        "    return dict(zip(tfidf_feature_names, tfidf_vectorizer.transform([doc]).toarray()[0]))\n",
        "\n",
        "tfidf_vectors = {doc: get_tfidf_vector(doc) for doc in documents}\n",
        "\n",
        "# Sentence Transformer Embeddings\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(documents, convert_to_numpy=True)\n",
        "embeddingsDict = {documents[i]: embedding for i,embedding in enumerate(embeddings)}\n",
        "\n",
        "test_doc = documents[0]\n",
        "print(f\"\"\"Document: '{test_doc}'\n",
        "                BM25 embedding (len: {len(bm25_scores[test_doc])}): {bm25_scores[test_doc]}\n",
        "              TF-IDF embedding (len: {len(tfidf_vectors[test_doc])}): {tfidf_vectors[test_doc]}\n",
        "Sentence Transformer embedding (len: {len(embeddingsDict[test_doc])}): {embeddingsDict[test_doc]}\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2T0ockN86cO"
      },
      "outputs": [],
      "source": [
        "for doc, emb in bm25_scores.items():\n",
        "  print(doc, emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZpPeR9T_KdH"
      },
      "outputs": [],
      "source": [
        "def apply_pca(embedding_dict, n_components=2):\n",
        "    \"\"\"Applies PCA to reduce the dimensionality of embeddings.\"\"\"\n",
        "    doc_keys = list(embedding_dict.keys())\n",
        "    vectors = np.array(list(embedding_dict.values()))\n",
        "\n",
        "    pca = PCA(n_components=n_components)\n",
        "    reduced_vectors = pca.fit_transform(vectors)\n",
        "\n",
        "    return {doc: reduced_vectors[i] for i, doc in enumerate(doc_keys)}\n",
        "\n",
        "# Apply PCA to each embedding method\n",
        "bm25_pca = apply_pca(bm25_scores)\n",
        "tfidf_pca = apply_pca({k: list(v.values()) for k,v in tfidf_vectors.items()})\n",
        "sentence_pca = apply_pca(embeddingsDict)\n",
        "\n",
        "# Print PCA reduced embeddings\n",
        "print(\"BM25 PCA Reduced:\", bm25_pca)\n",
        "print(\"TF-IDF PCA Reduced:\", tfidf_pca)\n",
        "print(\"Sentence Transformer PCA Reduced:\", sentence_pca)\n",
        "\n",
        "# Plot PCA results\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Extract coordinates\n",
        "def extract_coordinates(pca_dict):\n",
        "    return np.array(list(pca_dict.values()))\n",
        "\n",
        "bm25_coords = extract_coordinates(bm25_pca)\n",
        "tfidf_coords = extract_coordinates(tfidf_pca)\n",
        "sentence_coords = extract_coordinates(sentence_pca)\n",
        "\n",
        "plt.scatter(bm25_coords[:, 0], bm25_coords[:, 1], color='red', label='BM25')\n",
        "plt.scatter(tfidf_coords[:, 0], tfidf_coords[:, 1], color='blue', label='TF-IDF')\n",
        "plt.scatter(sentence_coords[:, 0], sentence_coords[:, 1], color='green', label='Sentence Transformer')\n",
        "\n",
        "# Annotate points\n",
        "for doc, coord in enumerate(bm25_pca.values()):\n",
        "    plt.annotate(doc, (coord[0], coord[1]), fontsize=9, color='red')\n",
        "for doc, coord in enumerate(tfidf_pca.values()):\n",
        "    plt.annotate(doc, (coord[0], coord[1]), fontsize=9, color='blue')\n",
        "for doc, coord in enumerate(sentence_pca.values()):\n",
        "    plt.annotate(doc, (coord[0], coord[1]), fontsize=9, color='green')\n",
        "\n",
        "plt.title(\"PCA Projection of Embeddings\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5wo9TBYLb71"
      },
      "source": [
        "# Search\n",
        "\n",
        "There are different ways to compare two vectors:\n",
        "![](https://pbs.twimg.com/media/GDTlkNqWsAA65BZ.jpg)\n",
        "\n",
        "\n",
        "Most common you'll probably see are euclidean and cosine similarity.\n",
        "\n",
        "Euclidean: the distance between the two 'points' / vector\n",
        "Cosine Similarity: the angle between the two 'points' / vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0k_z9AkRlIT"
      },
      "outputs": [],
      "source": [
        "doc2cosine_sim = {}\n",
        "doc2euclidean_dist = {}\n",
        "\n",
        "for doc_i in documents:\n",
        "  doc2cosine_sim[doc_i] = {}\n",
        "  doc2euclidean_dist[doc_i] = {}\n",
        "  for doc_j in documents:\n",
        "    if doc_j == doc_i:\n",
        "      continue\n",
        "    doc2cosine_sim[doc_i][doc_j] = cosine_similarity([embeddingsDict[doc_i]], [embeddingsDict[doc_j]])[0][0]\n",
        "    doc2euclidean_dist[doc_i][doc_j] = euclidean_distances([embeddingsDict[doc_i]], [embeddingsDict[doc_j]])[0][0]\n",
        "\n",
        "\n",
        "for doc, score_results in doc2cosine_sim.items():\n",
        "  print(doc)\n",
        "  for doc_j, score in score_results.items():\n",
        "    print(score, doc_j)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezKpW3aWSEeA"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "# index = faiss.IndexFlatL2(dimension) # Euclidean\n",
        "index = faiss.IndexFlatIP(dimension) # IP (Inner Product) - Cosine Similarity\n",
        "index.add(embeddings)\n",
        "\n",
        "# Sample Queries\n",
        "query = \"What did the patient present with?\"\n",
        "query = \"What were the presenting symptoms?\"\n",
        "query = \"What medications were given?\"\n",
        "\n",
        "query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "# Search for nearest neighbors\n",
        "distances, indices = index.search(query_embedding, k=2)\n",
        "\n",
        "# Print results\n",
        "print(\"Query:\", query)\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Match {i+1}: {documents[idx]} (Distance: {distances[0][i]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw2NEGI4Lt-R"
      },
      "source": [
        "# RAG\n",
        "\n",
        "Here we are using OpenAI to test for a few reasons:\n",
        "- great model\n",
        "- easy to use SDK (and very well documented)\n",
        "- don't have much hardware to run local open source models\n",
        "\n",
        "(Downloading and running Open source models is a conversation for another time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdPzVOkmS78b"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    api_key=\"\"\n",
        ")\n",
        "\n",
        "# Function to retrieve relevant context\n",
        "def retrieve_context(query, k=2):\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_embedding, k=k)\n",
        "    return [documents[i] for i in indices[0]]\n",
        "\n",
        "# Function to generate response using OpenAI\n",
        "def generate_response(query, client):\n",
        "    context = retrieve_context(query)\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Example query\n",
        "query = \"What did the patient present with?\"\n",
        "response = generate_response(query, client)\n",
        "print(\"Query:\", query)\n",
        "print(\"Response:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69TfIpzXLxR9"
      },
      "source": [
        "# Knowledge Graph\n",
        "\n",
        "![Knowledge Graph](https://valkyrie.ai/wp-content/uploads/2024/04/knowledgeGraphs_-1350x675-1.jpg)\n",
        "\n",
        "## Some Basic Steps for Constructing / Using\n",
        "1. Entity and Relationship Extraction (constructing)\n",
        "2. Ingestion into Graph (constructing)\n",
        "3. Graph Enrichment (constructing)\n",
        "\n",
        "4. Graph Querying (using)\n",
        "5. Agentic Graph (using)\n",
        "\n",
        "## Neo4j Information\n",
        "\n",
        "[Neo4j Documentation](https://neo4j.com/docs/cypher-manual/current/introduction/)\n",
        "\n",
        "[Indexes](https://neo4j.com/docs/cypher-manual/current/indexes/)\n",
        "\n",
        "[Full Text Index](https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/full-text-indexes/)\n",
        "\n",
        "[Neo4j Neo4j Python SDK](https://neo4j.com/docs/python-manual/current/)\n",
        "\n",
        "[Neo4j GenAI Python SDK](https://neo4j.com/docs/neo4j-graphrag-python/current/)\n",
        "\n",
        "[OpenAI Python SDK](https://github.com/openai/openai-python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nKxl41O9Toxv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph data ingested successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Neo4j connection details\n",
        "URI = \"bolt://localhost:7687\"\n",
        "AUTH = (\"neo4j\", \"password\") # NEED TO CHANGE\n",
        "\n",
        "def create_graph(driver):\n",
        "    with driver.session() as session:\n",
        "        session.run(\"MATCH (n) DETACH DELETE n\")  # Clear existing data\n",
        "\n",
        "        entities = [\n",
        "            {\"name\": \"Retrieval-Augmented Generation\", \"type\": \"Concept\", \"description\": \"A method that enhances LLM responses with document retrieval.\"},\n",
        "            {\"name\": \"FAISS\", \"type\": \"Tool\", \"description\": \"A library for efficient similarity search of embeddings.\"},\n",
        "            {\"name\": \"Sentence Transformers\", \"type\": \"Model\", \"description\": \"A model that converts text into dense embeddings.\"},\n",
        "            {\"name\": \"Neo4j\", \"type\": \"Database\", \"description\": \"A graph database used for knowledge graphs.\"}\n",
        "        ]\n",
        "\n",
        "        relationships = [\n",
        "            (\"Retrieval-Augmented Generation\", \"USES\", \"FAISS\"),\n",
        "            (\"Retrieval-Augmented Generation\", \"USES\", \"Sentence Transformers\"),\n",
        "            (\"Knowledge Graph\", \"STORES\", \"Neo4j\"),\n",
        "            (\"Neo4j\", \"SUPPORTS\", \"Graph Queries\")\n",
        "        ]\n",
        "\n",
        "        for entity in entities:\n",
        "            session.run(\n",
        "                f\"CREATE (n:{entity['type'].replace(' ', '_')} {{name: $name, description: $description}})\",\n",
        "                name=entity[\"name\"].replace(\" \", \"_\"),\n",
        "                description=entity[\"description\"]\n",
        "            )\n",
        "\n",
        "        for start, rel, end in relationships:\n",
        "            session.run(\n",
        "                \"MATCH (a {name: $start}), (b {name: $end}) \"\n",
        "                f\"CREATE (a)-[:{rel.replace(' ', '_')}]->(b)\",\n",
        "                start=start.replace(\" \", \"_\"),\n",
        "                end=end.replace(\" \", \"_\")\n",
        "            )\n",
        "\n",
        "        # create fulltext index\n",
        "        session.run(\n",
        "            \"CREATE FULLTEXT INDEX descriptionFulltextIndex IF NOT EXISTS FOR (n:Concept|Tool|Model|Database) ON EACH [n.description]\"\n",
        "        )\n",
        "\n",
        "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "create_graph(driver)\n",
        "print(\"Graph data ingested successfully.\")\n",
        "driver.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# other helpful queries\n",
        "QUERY_VIEW_INDEXES = \"show index\"\n",
        "QUERY_GET_ALL_NODE_TYPES = \"call db.labels()\"\n",
        "QUERY_SEE_ALL_FULLTEXT_ANALYZERS = \"db.index.fulltext.listAvailableAnalyzers\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByMjmPV7L4gR"
      },
      "source": [
        "# GraphRAG\n",
        "\n",
        "RAG (but with a Graph as your context source instead of a Vector Index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y-JBoL-Uo8W"
      },
      "outputs": [],
      "source": [
        "# Create fulltext index\n",
        "def create_fulltext_index():\n",
        "    with driver.session() as session:\n",
        "        session.run(\"\"\"\n",
        "        CREATE FULLTEXT INDEX entity_search IF NOT EXISTS\n",
        "        FOR (n:Entity) ON EACH [n.name, n.description]\n",
        "        \"\"\")\n",
        "\n",
        "def create_vector_index():\n",
        "    with driver.session() as session:\n",
        "        session.run(\"\"\"\n",
        "        CREATE INDEX vector_index IF NOT EXISTS\n",
        "        FOR (n:Entity) ON (n.embedding)\n",
        "        OPTIONS {indexProvider: 'vector-1.0', indexConfig: {`vector.dimensions`: 384, `vector.similarity_function`: 'cosine'}}\n",
        "        \"\"\")\n",
        "\n",
        "def retrieve_using_fulltext(query):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"\"\"\n",
        "        CALL db.index.fulltext.queryNodes('descriptionFulltextIndex', $fulltext_query) YIELD node, score\n",
        "        RETURN node.name, node.description, score\n",
        "        \"\"\", fulltext_query=query)\n",
        "\n",
        "        return result.data()\n",
        "    \n",
        "def retrieve_full_using_fulltext(query):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"\"\"\n",
        "        CALL db.index.fulltext.queryNodes('descriptionFulltextIndex', $fulltext_query)\n",
        "        YIELD node, score\n",
        "        MATCH (node)--(related)\n",
        "        RETURN \n",
        "            node.name AS name,\n",
        "            node.description AS description,\n",
        "            score,\n",
        "            collect(\n",
        "                DISTINCT {\n",
        "                    name: related.name, \n",
        "                    description: related.description\n",
        "                }\n",
        "            ) AS relatedNodes\n",
        "        \"\"\", fulltext_query=query)\n",
        "\n",
        "        return result.data()\n",
        "    \n",
        "def retrieve_using_vector(query):\n",
        "    query_embedding = model.encode(query, convert_to_numpy=True).tolist()\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"\"\"\n",
        "        MATCH (n:Entity)\n",
        "        RETURN n.name, n.description, cosineSimilarity(n.embedding, $query_embedding) AS score\n",
        "        ORDER BY score DESC LIMIT 2\n",
        "        \"\"\", query_embedding=query_embedding)\n",
        "        return result.data()\n",
        "\n",
        "# Setup indexes and ingest data\n",
        "# create_fulltext_index()\n",
        "# create_vector_index()\n",
        "\n",
        "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "\n",
        "# Example query\n",
        "query = \"How does retrieval-augmented generation work?\"\n",
        "fulltext_results = retrieve_using_fulltext(query)\n",
        "# vector_results = retrieve_using_vector(query)\n",
        "\n",
        "fulltext_full_results = retrieve_full_using_fulltext(query)\n",
        "\n",
        "print(\"Fulltext Search Results:\", fulltext_results)\n",
        "# print(\"Vector Search Results:\", vector_results)\n",
        "\n",
        "print(\"Fulltext FULL Search Results:\", fulltext_full_results)\n",
        "driver.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8UQ5ZMF26ZW"
      },
      "outputs": [],
      "source": [
        "# Function to retrieve relevant context from graph\n",
        "def retrieve_graph_context(query, fulltext_fn=retrieve_full_using_fulltext):\n",
        "    fulltext_full_results = fulltext_fn(query)\n",
        "    return fulltext_full_results\n",
        "\n",
        "# Function to generate response using OpenAI\n",
        "def generate_response(query, client, context_fn=retrieve_graph_context):\n",
        "    context = context_fn(query)\n",
        "    print(f\"CONTEXT: {context}\")\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant. You will probably receive context to generate a response. Rely on context to answer the question.\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "\n",
        "# Example query\n",
        "query = \"How does retrieval-augmented generation work?\"\n",
        "response = generate_response(query, client)\n",
        "print(\"Query:\", query)\n",
        "print(\"Response:\", response)\n",
        "\n",
        "driver.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# No context response\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant. You will probably receive context to generate a response. Rely on context to answer the question.\"},\n",
        "                {\"role\": \"user\", \"content\": query}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# agent toolbox\n",
        "\n",
        "def query_neo4j(cypher_query: str, parameters: dict = None) -> list:\n",
        "    \"\"\"\n",
        "    Execute a Cypher query on the Neo4j database.\n",
        "    Returns a list of records.\n",
        "    Each record is a dictionary-like object containing key-value pairs of returned fields.\n",
        "    \"\"\"\n",
        "    if parameters is None:\n",
        "        parameters = {}\n",
        "    with driver.session() as session:\n",
        "        result = session.run(cypher_query, **parameters)\n",
        "        return [record.data() for record in result]\n",
        "    \n",
        "def query_index(index_name: str, query: str) -> list:\n",
        "    cypher_query = (\n",
        "        f\"CALL db.index.fulltext.queryNodes('{index_name}', '{query}') YIELD node, score \"\n",
        "        \"RETURN node, score\"\n",
        "    )\n",
        "    with driver.session() as session:\n",
        "        result = session.run(cypher_query.format(index_name=index_name, query=query))\n",
        "        return [record.data() for record in result]\n",
        "\n",
        "def search_graph(cypher_query: str) -> str:\n",
        "    \"\"\"\n",
        "    The agent can call this function with a Cypher query string.\n",
        "    We'll pass it to the Neo4j database and return the result as a JSON string.\n",
        "    \"\"\"\n",
        "    results = query_neo4j(cypher_query)\n",
        "    return json.dumps(results, indent=2)\n",
        "\n",
        "def search_index(index_name: str, query: str) -> str:\n",
        "    results = query_index(index_name=index_name, query=query)\n",
        "    return json.dumps(results, indent=2)\n",
        "\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"search_graph\",\n",
        "        \"description\": \"Execute a Cypher query on the Neo4j database and return the results.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"cypher_query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"A valid Cypher query to execute.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"cypher_query\"]\n",
        "        }\n",
        "    },\n",
        "        {\n",
        "        \"name\": \"search_index\",\n",
        "        \"description\": \"Execute a search query on a Fulltext index in the Neo4j database and return the results.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"index_name\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"A fulltext index name.\"\n",
        "                },\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"A query to run.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"index_name\", \"query\"]\n",
        "        }\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to query a 'graph' agent.\n",
        "# The agent has access to the graph db to answer questions.\n",
        "\n",
        "def ask_graph_agent(user_question: str) -> str:\n",
        "    print(f\"- NEW QUERY: {user_question}\")\n",
        "    # Step 1: Send the user question + function definitions to the model\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a medical record assistant. You have access to a function that can run Cypher queries on \"\n",
        "                \"a Neo4j knowledge graph of a synthetic patient record. You should generate relevant queries to \"\n",
        "                \"fetch patient information. Then return the final answer in natural language. \"\n",
        "                \"# SCHEMA INFORMATION \"\n",
        "                \"- All Node Types: ['Diagnosis', 'Sign/Symptom', 'Medication', 'Procedure', 'Measurement', 'Page'] \"\n",
        "                \"- All nodes have primary field of 'text' (except 'Page' nodes have 'page_text') \"\n",
        "                \"# INDEXES \"\n",
        "                \"- 'Diagnosis_FULLTEXT_index' on 'Diagnosis' nodes on 'text' field \"\n",
        "                \"- 'Page_FULLTEXT_index' on 'Page' nodes on 'page_text' field\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_question\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\"  # Let the model decide when to call the function\n",
        "    )\n",
        "    \n",
        "    # Step 2: Check if the model wants to call a function\n",
        "    response_message = response.choices[0].message\n",
        "    \n",
        "    if response_message.function_call:\n",
        "        # The model requested to call a function\n",
        "        function_name = response_message.function_call.name\n",
        "        function_args = json.loads(response_message.function_call.arguments)\n",
        "\n",
        "        print(f\"- NEW FN CALL: {function_name} | {function_args}\")\n",
        "        \n",
        "        if function_name == \"search_graph\":\n",
        "            # Step 3: Execute the function\n",
        "            function_result = search_graph(**function_args)  # pass the cypher_query\n",
        "\n",
        "            print(f\"\\t- FN RESULT: {function_result}\")\n",
        "            \n",
        "            # Step 4: Return that result back to the model to get a final answer\n",
        "            messages.append(response_message)  # append the function call\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"role\": \"function\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_result\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            second_response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=messages\n",
        "            )\n",
        "            \n",
        "            final_answer = second_response.choices[0].message.content\n",
        "            \n",
        "            print(f\"- FINAL RESP: {final_answer}\")\n",
        "\n",
        "            return final_answer\n",
        "        elif function_name == \"search_index\":\n",
        "            # Step 3: Execute the function\n",
        "            function_result = search_index(**function_args)  # pass the arguments\n",
        "\n",
        "            print(f\"\\t- FN RESULT: {function_result}\")\n",
        "            \n",
        "            # Step 4: Return that result back to the model to get a final answer\n",
        "            messages.append(response_message)  # append the function call\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"role\": \"function\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_result\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            second_response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=messages\n",
        "            )\n",
        "            \n",
        "            final_answer = second_response.choices[0].message.content\n",
        "            \n",
        "            print(f\"- FINAL RESP: {final_answer}\")\n",
        "\n",
        "            return final_answer\n",
        "        else:\n",
        "            # If there's some other function, handle accordingly\n",
        "            pass\n",
        "    \n",
        "    # Otherwise, if no function call is requested, just return the raw content\n",
        "    return response_message[\"content\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "\n",
        "query1 = \"What medications were given to the patient?\"\n",
        "query2 = \"What diagnoses does the patient have?\"\n",
        "query3 = \"What page numbers was documented for levaquin?\"\n",
        "\n",
        "ask_graph_agent(query1)\n",
        "\n",
        "driver.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
