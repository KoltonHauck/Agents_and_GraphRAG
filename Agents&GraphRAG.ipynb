{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MElV2aFFK5Lq"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KoltonHauck/BMI6016_VectorDB/blob/main/BMI6016-VectorDB.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtDJGOem3nPZ"
      },
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkiFA7e53ZDw",
        "outputId": "c9e758b4-f9a1-467b-bec3-751f4b582796"
      },
      "outputs": [],
      "source": [
        "pip install openai numpy pandas scikit-learn sentence-transformers rank-bm25 faiss-cpu neo4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBgd7kNK--K"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tdaosZCLZ1B"
      },
      "source": [
        "# Embedding Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JFi6G_LQq9e",
        "outputId": "6b1a261e-8ed8-44fb-a9e7-93a21035c6e8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Sample corpus\n",
        "documents = [\n",
        "    \"RAG uses retrieval-augmented generation to enhance responses.\",\n",
        "    \"BM25 is a ranking function used in information retrieval.\",\n",
        "    \"TF-IDF measures the importance of terms in a document.\",\n",
        "    \"Sentence transformers convert text into dense vector embeddings.\",\n",
        "    \"LLMs can use embeddings for semantic search.\"\n",
        "]\n",
        "\n",
        "# BM25 Embeddings\n",
        "tokenized_corpus = [doc.split(\" \") for doc in documents]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "bm25_scores = {doc: bm25.get_scores(doc.split(\" \")) for doc in documents}\n",
        "\n",
        "# TF-IDF Embeddings\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents).toarray()\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "def get_tfidf_vector(doc):\n",
        "    return dict(zip(tfidf_feature_names, tfidf_vectorizer.transform([doc]).toarray()[0]))\n",
        "\n",
        "tfidf_vectors = {doc: get_tfidf_vector(doc) for doc in documents}\n",
        "\n",
        "# Sentence Transformer Embeddings\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(documents, convert_to_numpy=True)\n",
        "embeddingsDict = {documents[i]: embedding for i,embedding in enumerate(embeddings)}\n",
        "\n",
        "test_doc = documents[0]\n",
        "print(f\"\"\"Document: '{test_doc}'\n",
        "                BM25 embedding (len: {len(bm25_scores[test_doc])}): {bm25_scores[test_doc]}\n",
        "              TF-IDF embedding (len: {len(tfidf_vectors[test_doc])}): {tfidf_vectors[test_doc]}\n",
        "Sentence Transformer embedding (len: {len(embeddingsDict[test_doc])}): {embeddingsDict[test_doc]}\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2T0ockN86cO",
        "outputId": "46b91909-30e3-4b04-f2d0-48b99761f9e2"
      },
      "outputs": [],
      "source": [
        "for doc, emb in bm25_scores.items():\n",
        "  print(doc, emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "kZpPeR9T_KdH",
        "outputId": "ee91aa3b-5eb5-4dd4-d9d2-e32ec2eb40bc"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def apply_pca(embedding_dict, n_components=2):\n",
        "    \"\"\"Applies PCA to reduce the dimensionality of embeddings.\"\"\"\n",
        "    doc_keys = list(embedding_dict.keys())\n",
        "    vectors = np.array(list(embedding_dict.values()))\n",
        "\n",
        "    pca = PCA(n_components=n_components)\n",
        "    reduced_vectors = pca.fit_transform(vectors)\n",
        "\n",
        "    return {doc: reduced_vectors[i] for i, doc in enumerate(doc_keys)}\n",
        "\n",
        "# Apply PCA to each embedding method\n",
        "bm25_pca = apply_pca(bm25_scores)\n",
        "tfidf_pca = apply_pca({k: list(v.values()) for k,v in tfidf_vectors.items()})\n",
        "sentence_pca = apply_pca(embeddingsDict)\n",
        "\n",
        "# Print PCA reduced embeddings\n",
        "print(\"BM25 PCA Reduced:\", bm25_pca)\n",
        "print(\"TF-IDF PCA Reduced:\", tfidf_pca)\n",
        "print(\"Sentence Transformer PCA Reduced:\", sentence_pca)\n",
        "\n",
        "# Plot PCA results\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Extract coordinates\n",
        "def extract_coordinates(pca_dict):\n",
        "    return np.array(list(pca_dict.values()))\n",
        "\n",
        "bm25_coords = extract_coordinates(bm25_pca)\n",
        "tfidf_coords = extract_coordinates(tfidf_pca)\n",
        "sentence_coords = extract_coordinates(sentence_pca)\n",
        "\n",
        "plt.scatter(bm25_coords[:, 0], bm25_coords[:, 1], color='red', label='BM25')\n",
        "plt.scatter(tfidf_coords[:, 0], tfidf_coords[:, 1], color='blue', label='TF-IDF')\n",
        "plt.scatter(sentence_coords[:, 0], sentence_coords[:, 1], color='green', label='Sentence Transformer')\n",
        "\n",
        "# Annotate points\n",
        "for doc, coord in bm25_pca.items():\n",
        "    plt.annotate(doc[:2], (coord[0], coord[1]), fontsize=9, color='red')\n",
        "for doc, coord in tfidf_pca.items():\n",
        "    plt.annotate(doc[:2], (coord[0], coord[1]), fontsize=9, color='blue')\n",
        "for doc, coord in sentence_pca.items():\n",
        "    plt.annotate(doc[:2], (coord[0], coord[1]), fontsize=9, color='green')\n",
        "\n",
        "plt.title(\"PCA Projection of Embeddings\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tZaO2w8B0YY"
      },
      "outputs": [],
      "source": [
        "# show embedding math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5wo9TBYLb71"
      },
      "source": [
        "# Search\n",
        "\n",
        "- cosine sim\n",
        "- euclidean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0k_z9AkRlIT",
        "outputId": "5816faff-9e68-4734-fe84-2d9b70702f3f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "doc2cosine_sim = {}\n",
        "doc2euclidean_dist = {}\n",
        "\n",
        "for doc_i in documents:\n",
        "  doc2cosine_sim[doc_i] = {}\n",
        "  doc2euclidean_dist[doc_i] = {}\n",
        "  for doc_j in documents:\n",
        "    if doc_j == doc_i:\n",
        "      continue\n",
        "    doc2cosine_sim[doc_i][doc_j] = cosine_similarity([embeddingsDict[doc_i]], [embeddingsDict[doc_j]])[0][0]\n",
        "    doc2euclidean_dist[doc_i][doc_j] = euclidean_distances([embeddingsDict[doc_i]], [embeddingsDict[doc_j]])[0][0]\n",
        "\n",
        "\n",
        "for doc, score_results in doc2cosine_sim.items():\n",
        "  print(doc)\n",
        "  for doc_j, score in score_results.items():\n",
        "    print(score, doc_j)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKxSU5V-CNY5"
      },
      "outputs": [],
      "source": [
        "# rank and sort relevant 'documents' based on score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezKpW3aWSEeA",
        "outputId": "0094e3ac-11cc-49ae-959f-60109c497071"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# Query example\n",
        "query = \"How do sentence transformers work?\"\n",
        "query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "# Search for nearest neighbors\n",
        "distances, indices = index.search(query_embedding, k=2)\n",
        "\n",
        "# Print results\n",
        "print(\"Query:\", query)\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Match {i+1}: {documents[idx]} (Distance: {distances[0][i]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw2NEGI4Lt-R"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdPzVOkmS78b",
        "outputId": "d5733da9-c09c-4d89-9bf1-461374bf259c"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "client = OpenAI(\n",
        "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    api_key=\"\"\n",
        ")\n",
        "\n",
        "# Function to retrieve relevant context\n",
        "def retrieve_context(query, k=2):\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_embedding, k=k)\n",
        "    return [documents[i] for i in indices[0]]\n",
        "\n",
        "# Function to generate response using OpenAI\n",
        "def generate_response(query, client):\n",
        "    context = retrieve_context(query)\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Example query\n",
        "query = \"How do sentence transformers work?\"\n",
        "response = generate_response(query, client)\n",
        "print(\"Query:\", query)\n",
        "print(\"Response:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69TfIpzXLxR9"
      },
      "source": [
        "# Knowledge Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKxl41O9Toxv"
      },
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Neo4j connection details\n",
        "URI = \"bolt://localhost:7687\"  # Change as needed\n",
        "AUTH = (\"neo4j\", \"password\")  # Replace with actual credentials\n",
        "\n",
        "def create_graph(driver):\n",
        "    with driver.session() as session:\n",
        "        session.run(\"MATCH (n) DETACH DELETE n\")  # Clear existing data\n",
        "\n",
        "        entities = [\n",
        "            {\"name\": \"Retrieval-Augmented Generation\", \"type\": \"Concept\"},\n",
        "            {\"name\": \"FAISS\", \"type\": \"Tool\"},\n",
        "            {\"name\": \"Sentence Transformers\", \"type\": \"Model\"},\n",
        "            {\"name\": \"Neo4j\", \"type\": \"Database\"}\n",
        "        ]\n",
        "\n",
        "        relationships = [\n",
        "            (\"Retrieval-Augmented Generation\", \"USES\", \"FAISS\"),\n",
        "            (\"Retrieval-Augmented Generation\", \"USES\", \"Sentence Transformers\"),\n",
        "            (\"Knowledge Graph\", \"STORES\", \"Neo4j\"),\n",
        "            (\"Neo4j\", \"SUPPORTS\", \"Graph Queries\")\n",
        "        ]\n",
        "\n",
        "        for entity in entities:\n",
        "            session.run(\n",
        "                \"CREATE (n:{type} {{name: $name}})\",\n",
        "                name=entity[\"name\"],\n",
        "                type=entity[\"type\"]\n",
        "            )\n",
        "\n",
        "        for start, rel, end in relationships:\n",
        "            session.run(\n",
        "                \"MATCH (a {name: $start}), (b {name: $end}) \"\n",
        "                \"CREATE (a)-[:{rel}]->(b)\",\n",
        "                start=start,\n",
        "                rel=rel,\n",
        "                end=end\n",
        "            )\n",
        "\n",
        "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "create_graph(driver)\n",
        "print(\"Graph data ingested successfully.\")\n",
        "driver.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByMjmPV7L4gR"
      },
      "source": [
        "# GraphRAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y-JBoL-Uo8W"
      },
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Neo4j connection details\n",
        "URI = \"bolt://localhost:7687\"  # Adjust as needed\n",
        "AUTH = (\"neo4j\", \"password\")  # Replace with actual credentials\n",
        "\n",
        "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "\n",
        "# Create fulltext index\n",
        "def create_fulltext_index():\n",
        "    with driver.session() as session:\n",
        "        session.run(\"\"\"\n",
        "        CREATE FULLTEXT INDEX entity_search IF NOT EXISTS\n",
        "        FOR (n:Entity) ON EACH [n.name, n.description]\n",
        "        \"\"\")\n",
        "\n",
        "def create_vector_index():\n",
        "    with driver.session() as session:\n",
        "        session.run(\"\"\"\n",
        "        CREATE INDEX vector_index IF NOT EXISTS\n",
        "        FOR (n:Entity) ON (n.embedding)\n",
        "        OPTIONS {indexProvider: 'vector-1.0', indexConfig: {`vector.dimensions`: 384, `vector.similarity_function`: 'cosine'}}\n",
        "        \"\"\")\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    {\"name\": \"Retrieval-Augmented Generation\", \"description\": \"A method that enhances LLM responses with document retrieval.\"},\n",
        "    {\"name\": \"FAISS\", \"description\": \"A library for efficient similarity search of embeddings.\"},\n",
        "    {\"name\": \"Sentence Transformers\", \"description\": \"A model that converts text into dense embeddings.\"},\n",
        "    {\"name\": \"Neo4j\", \"description\": \"A graph database used for knowledge graphs.\"}\n",
        "]\n",
        "\n",
        "# Load embedding model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def ingest_data():\n",
        "    with driver.session() as session:\n",
        "        for doc in documents:\n",
        "            embedding = model.encode(doc[\"description\"], convert_to_numpy=True).tolist()\n",
        "            session.run(\"\"\"\n",
        "            CREATE (n:Entity {name: $name, description: $description, embedding: $embedding})\n",
        "            \"\"\", name=doc[\"name\"], description=doc[\"description\"], embedding=embedding)\n",
        "\n",
        "def retrieve_using_fulltext(query):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"\"\"\n",
        "        CALL db.index.fulltext.queryNodes('entity_search', $query) YIELD node, score\n",
        "        RETURN node.name, node.description, score\n",
        "        \"\"\", query=query)\n",
        "        return result.data()\n",
        "\n",
        "def retrieve_using_vector(query):\n",
        "    query_embedding = model.encode(query, convert_to_numpy=True).tolist()\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"\"\"\n",
        "        MATCH (n:Entity)\n",
        "        RETURN n.name, n.description, cosineSimilarity(n.embedding, $query_embedding) AS score\n",
        "        ORDER BY score DESC LIMIT 2\n",
        "        \"\"\", query_embedding=query_embedding)\n",
        "        return result.data()\n",
        "\n",
        "# Setup indexes and ingest data\n",
        "create_fulltext_index()\n",
        "create_vector_index()\n",
        "ingest_data()\n",
        "\n",
        "# Example query\n",
        "query = \"How does retrieval-augmented generation work?\"\n",
        "fulltext_results = retrieve_using_fulltext(query)\n",
        "vector_results = retrieve_using_vector(query)\n",
        "\n",
        "print(\"Fulltext Search Results:\", fulltext_results)\n",
        "print(\"Vector Search Results:\", vector_results)\n",
        "\n",
        "driver.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3JoKbC826ZW"
      },
      "source": [
        "# Graph Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8UQ5ZMF26ZW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
