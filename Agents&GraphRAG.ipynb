{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MElV2aFFK5Lq"
      },
      "source": [
        "\n",
        "\n",
        "https://colab.research.google.com/github/KoltonHauck/Agents_and_GraphRAG/blob/main/Agents&GraphRAG.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBgd7kNK--K"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tdaosZCLZ1B"
      },
      "source": [
        "# Embedding Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JFi6G_LQq9e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Sample corpus\n",
        "documents = [\n",
        "    \"RAG uses retrieval-augmented generation to enhance responses.\",\n",
        "    \"BM25 is a ranking function used in information retrieval.\",\n",
        "    \"TF-IDF measures the importance of terms in a document.\",\n",
        "    \"Sentence transformers convert text into dense vector embeddings.\",\n",
        "    \"LLMs can use embeddings for semantic search.\"\n",
        "]\n",
        "\n",
        "# BM25 Embeddings\n",
        "tokenized_corpus = [doc.split(\" \") for doc in documents]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "bm25_scores = {doc: bm25.get_scores(doc.split(\" \")) for doc in documents}\n",
        "\n",
        "# TF-IDF Embeddings\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents).toarray()\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "def get_tfidf_vector(doc):\n",
        "    return dict(zip(tfidf_feature_names, tfidf_vectorizer.transform([doc]).toarray()[0]))\n",
        "\n",
        "tfidf_vectors = {doc: get_tfidf_vector(doc) for doc in documents}\n",
        "\n",
        "# Sentence Transformer Embeddings\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "def print_embeddings():\n",
        "    print(\"\\nBM25 Scores:\")\n",
        "    for doc, scores in bm25_scores.items():\n",
        "        print(f\"{doc}: {scores}\")\n",
        "\n",
        "    print(\"\\nTF-IDF Vectors:\")\n",
        "    for doc, vector in tfidf_vectors.items():\n",
        "        print(f\"{doc}: {vector}\")\n",
        "\n",
        "    print(\"\\nSentence Transformer Embeddings:\")\n",
        "    for doc, emb in zip(documents, embeddings):\n",
        "        print(f\"{doc}: {emb[:5]}...\")  # Print first 5 dimensions for brevity\n",
        "\n",
        "print_embeddings()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5wo9TBYLb71"
      },
      "source": [
        "# Search\n",
        "\n",
        "- cosine sim\n",
        "- euclidean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0k_z9AkRlIT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Sample sentences\n",
        "sentence1 = \"RAG enhances response generation using retrieval.\"\n",
        "sentence2 = \"Retrieval-augmented generation improves answer quality.\"\n",
        "\n",
        "# Load Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Compute embeddings\n",
        "embedding1 = model.encode(sentence1, convert_to_numpy=True)\n",
        "embedding2 = model.encode(sentence2, convert_to_numpy=True)\n",
        "\n",
        "# Compute similarity metrics\n",
        "cosine_sim = cosine_similarity([embedding1], [embedding2])[0][0]\n",
        "euclidean_dist = euclidean_distances([embedding1], [embedding2])[0][0]\n",
        "\n",
        "# Print results\n",
        "print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n",
        "print(f\"Euclidean Distance: {euclidean_dist:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6VM6M8wLqfu"
      },
      "source": [
        "# Vector Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezKpW3aWSEeA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Sample corpus\n",
        "documents = [\n",
        "    \"RAG uses retrieval-augmented generation to enhance responses.\",\n",
        "    \"BM25 is a ranking function used in information retrieval.\",\n",
        "    \"TF-IDF measures the importance of terms in a document.\",\n",
        "    \"Sentence transformers convert text into dense vector embeddings.\",\n",
        "    \"LLMs can use embeddings for semantic search.\"\n",
        "]\n",
        "\n",
        "# Load Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Compute embeddings\n",
        "embeddings = model.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# Query example\n",
        "query = \"How do sentence transformers work?\"\n",
        "query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "# Search for nearest neighbors\n",
        "distances, indices = index.search(query_embedding, k=2)\n",
        "\n",
        "# Print results\n",
        "print(\"Query:\", query)\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Match {i+1}: {documents[idx]} (Distance: {distances[0][i]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw2NEGI4Lt-R"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdPzVOkmS78b"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Sample corpus\n",
        "documents = [\n",
        "    \"RAG uses retrieval-augmented generation to enhance responses.\",\n",
        "    \"BM25 is a ranking function used in information retrieval.\",\n",
        "    \"TF-IDF measures the importance of terms in a document.\",\n",
        "    \"Sentence transformers convert text into dense vector embeddings.\",\n",
        "    \"LLMs can use embeddings for semantic search.\"\n",
        "]\n",
        "\n",
        "# Load Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Compute embeddings\n",
        "embeddings = model.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# Function to retrieve relevant context\n",
        "def retrieve_context(query, k=2):\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_embedding, k=k)\n",
        "    return [documents[i] for i in indices[0]]\n",
        "\n",
        "# Function to generate response using OpenAI\n",
        "def generate_response(query):\n",
        "    context = retrieve_context(query)\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# Example query\n",
        "query = \"How do sentence transformers work?\"\n",
        "response = generate_response(query)\n",
        "print(\"Query:\", query)\n",
        "print(\"Response:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69TfIpzXLxR9"
      },
      "source": [
        "# Knowledge Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKxl41O9Toxv"
      },
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Neo4j connection details\n",
        "URI = \"bolt://localhost:7687\"  # Change as needed\n",
        "AUTH = (\"neo4j\", \"password\")  # Replace with actual credentials\n",
        "\n",
        "def create_graph(driver):\n",
        "    with driver.session() as session:\n",
        "        session.run(\"MATCH (n) DETACH DELETE n\")  # Clear existing data\n",
        "\n",
        "        entities = [\n",
        "            {\"name\": \"Retrieval-Augmented Generation\", \"type\": \"Concept\"},\n",
        "            {\"name\": \"FAISS\", \"type\": \"Tool\"},\n",
        "            {\"name\": \"Sentence Transformers\", \"type\": \"Model\"},\n",
        "            {\"name\": \"Neo4j\", \"type\": \"Database\"}\n",
        "        ]\n",
        "\n",
        "        relationships = [\n",
        "            (\"Retrieval-Augmented Generation\", \"USES\", \"FAISS\"),\n",
        "            (\"Retrieval-Augmented Generation\", \"USES\", \"Sentence Transformers\"),\n",
        "            (\"Knowledge Graph\", \"STORES\", \"Neo4j\"),\n",
        "            (\"Neo4j\", \"SUPPORTS\", \"Graph Queries\")\n",
        "        ]\n",
        "\n",
        "        for entity in entities:\n",
        "            session.run(\n",
        "                \"CREATE (n:{type} {{name: $name}})\",\n",
        "                name=entity[\"name\"],\n",
        "                type=entity[\"type\"]\n",
        "            )\n",
        "\n",
        "        for start, rel, end in relationships:\n",
        "            session.run(\n",
        "                \"MATCH (a {name: $start}), (b {name: $end}) \"\n",
        "                \"CREATE (a)-[:{rel}]->(b)\",\n",
        "                start=start,\n",
        "                rel=rel,\n",
        "                end=end\n",
        "            )\n",
        "\n",
        "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "create_graph(driver)\n",
        "print(\"Graph data ingested successfully.\")\n",
        "driver.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByMjmPV7L4gR"
      },
      "source": [
        "# GraphRAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y-JBoL-Uo8W"
      },
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Neo4j connection details\n",
        "URI = \"bolt://localhost:7687\"  # Adjust as needed\n",
        "AUTH = (\"neo4j\", \"password\")  # Replace with actual credentials\n",
        "\n",
        "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "\n",
        "# Create fulltext index\n",
        "def create_fulltext_index():\n",
        "    with driver.session() as session:\n",
        "        session.run(\"\"\"\n",
        "        CREATE FULLTEXT INDEX entity_search IF NOT EXISTS\n",
        "        FOR (n:Entity) ON EACH [n.name, n.description]\n",
        "        \"\"\")\n",
        "\n",
        "def create_vector_index():\n",
        "    with driver.session() as session:\n",
        "        session.run(\"\"\"\n",
        "        CREATE INDEX vector_index IF NOT EXISTS\n",
        "        FOR (n:Entity) ON (n.embedding)\n",
        "        OPTIONS {indexProvider: 'vector-1.0', indexConfig: {`vector.dimensions`: 384, `vector.similarity_function`: 'cosine'}}\n",
        "        \"\"\")\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    {\"name\": \"Retrieval-Augmented Generation\", \"description\": \"A method that enhances LLM responses with document retrieval.\"},\n",
        "    {\"name\": \"FAISS\", \"description\": \"A library for efficient similarity search of embeddings.\"},\n",
        "    {\"name\": \"Sentence Transformers\", \"description\": \"A model that converts text into dense embeddings.\"},\n",
        "    {\"name\": \"Neo4j\", \"description\": \"A graph database used for knowledge graphs.\"}\n",
        "]\n",
        "\n",
        "# Load embedding model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def ingest_data():\n",
        "    with driver.session() as session:\n",
        "        for doc in documents:\n",
        "            embedding = model.encode(doc[\"description\"], convert_to_numpy=True).tolist()\n",
        "            session.run(\"\"\"\n",
        "            CREATE (n:Entity {name: $name, description: $description, embedding: $embedding})\n",
        "            \"\"\", name=doc[\"name\"], description=doc[\"description\"], embedding=embedding)\n",
        "\n",
        "def retrieve_using_fulltext(query):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"\"\"\n",
        "        CALL db.index.fulltext.queryNodes('entity_search', $query) YIELD node, score\n",
        "        RETURN node.name, node.description, score\n",
        "        \"\"\", query=query)\n",
        "        return result.data()\n",
        "\n",
        "def retrieve_using_vector(query):\n",
        "    query_embedding = model.encode(query, convert_to_numpy=True).tolist()\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"\"\"\n",
        "        MATCH (n:Entity)\n",
        "        RETURN n.name, n.description, cosineSimilarity(n.embedding, $query_embedding) AS score\n",
        "        ORDER BY score DESC LIMIT 2\n",
        "        \"\"\", query_embedding=query_embedding)\n",
        "        return result.data()\n",
        "\n",
        "# Setup indexes and ingest data\n",
        "create_fulltext_index()\n",
        "create_vector_index()\n",
        "ingest_data()\n",
        "\n",
        "# Example query\n",
        "query = \"How does retrieval-augmented generation work?\"\n",
        "fulltext_results = retrieve_using_fulltext(query)\n",
        "vector_results = retrieve_using_vector(query)\n",
        "\n",
        "print(\"Fulltext Search Results:\", fulltext_results)\n",
        "print(\"Vector Search Results:\", vector_results)\n",
        "\n",
        "driver.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
